run:
  batch_size: 2
  total_samples: 1024
  target_languages: ["morocco", "msa", "gulf", "english", "levantine"] # list of target languages; total_samples is generated per language
  run_id: ""
  resume_run_id: ""
  turns:
    mode: range # exact | range
    exact: null
    min: 2
    max: 12
    distribution: poisson # uniform | poisson | exponential
    mean: 6.0
  data:
    seeding:
      question: ""
      topics:
        path: data/seeds/topics.yaml
        enabled: false
        variant: ""
        probability: 0.35
  distributed:
    enabled: false
    backend: ray
    spawn:
      coordinator: true
      workers: true
    ray:
      address: auto
      auto_start_local: true # when true and ray.address=auto finds no cluster, dlgforge starts a local Ray runtime
      namespace: dlgforge
      actor:
        num_cpus: 1.0
        num_gpus: 0.0
        coordinator_num_cpus: 1.0
        replicas_qa: 2
        replicas_complete: 2

llm:
  mode: api # api | vllm_attach | vllm_managed
  routing:
    strategy: weighted_least_inflight
    # required for llm.mode=vllm_attach when you already have a vllm cluster running; not needed for llm.mode=vllm_managed as the cluster will be created and managed by dlgforge, not needed when llm.mode=api
    # endpoints: ["http://localhost:1234"] # for LM Studio
    # endpoints: ["http://localhost:8000"] # for vLLM
  vllm:
    model: openai/gpt-oss-20b
    served_model_name: gpt_oss_20b
    replicas: 1
    num_gpus_per_replica: 1
    host: 0.0.0.0
    advertise_host: 127.0.0.1
    port_start: 18000
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_num_seqs: 256
    health_timeout_s: 180
    auto_stop_on_exit: true
  agents:
    # refer to https://docs.litellm.ai/docs/providers for base_url and api_key formats for different providers
    user:
      provider: openai
      model: gpt-5.2 # gpt-5.2 , "openai/gpt-oss-20b"
      base_url: https://api.openai.com/v1
      temperature: null
      max_tokens: null
      top_p: null
      timeout: null
      max_retries: null
      extra: {}
    assistant:
      provider: gemini
      model: gemini/gemini-3-flash-preview # gpt-5.2 , "openai/gpt-oss-20b"
      base_url: ""
      temperature: null
      max_tokens: null
      top_p: null
      timeout: null
      max_retries: null
      extra: {}
    judge: # judge is ignored when judge.mode: offline in config.yaml.
      provider: gemini
      model: gemini/gemini-3-flash-preview #gpt-5.2 , "openai/gpt-oss-20b"
      base_url: ""
      temperature: null
      max_tokens: null
      top_p: null
      timeout: null
      max_retries: null
      extra: {}

store:
  backend: postgres
  postgres:
    dsn: ""
    min_pool_size: 5
    max_pool_size: 30
    statement_timeout_ms: 30000


coverage:
  doc_coverage_mode: balanced
  doc_coverage_epsilon: 0.15
  doc_coverage_fraction: 0.2
  question_dedup_retries: 3

tools:
  web_search:
    enabled: false
    serper_num_results: 5
    serper_timeout: 30
  retrieval:
    top_k: 4
    chunking:
      chunk_size: 750
      chunk_overlap: 150
    index:
      persist_dir: knowledge_index
      rebuild: false
      skip_if_unchanged: true
    embeddings:
      backend: sentence_transformers
      model: sentence-transformers/all-MiniLM-L6-v2
      fallback_model: sentence-transformers/all-MiniLM-L6-v2
      device: auto
      fallback_on_cpu: true
      model_kwargs: {}
      tokenizer_kwargs: {}
      encode_kwargs: {}
    reranker:
      enabled: false
      model: Qwen/Qwen3-Reranker-4B
      backend: qwen3
      instruction: >
        Given a user question and a candidate passage from the knowledge base,
        decide whether the passage contains information needed to answer the question.
      max_length: 8192
      candidates: 12
      batch_size: 16
      cmd: null

personas:
  enabled: true
  path: src/dlgforge/prompts/personas.yaml

judge:
  enabled: true
  mode: online # online will make the judging in generation time, offline will not and will run separatly
  granularity: turn # turn or conversation
  reasons:
    - irrelevant
    - incorrect
    - hallucinated
    - weak_grounding
    - vague
    - incomplete
    - unsafe
    - other

saving:
  output_dir: outputs
  # output_dir: outputs_local
  output_columns:
    messages: messages
    messages_with_tools: messages_with_tools
    metadata: metadata
    user_reasoning: user_reasoning
    assistant_reasoning: assistant_reasoning
    judge: judge
  hf_push:
    enabled: true
    auto_push_on_run: true
    repo_id: MBZUAI-Paris/Synthetic-QA-Reasoning-Multilingual
    # repo_id: MBZUAI-Paris/Synthetic-QA-Reasoning-local
    repo_type: dataset
    export_dir: hf_export
    include_run_state: false
    private: true
    commit_message: Update synthetic dataset export
    # source_file: conversations_sharegpt_judged.jsonl
    source_file: conversations_sharegpt.jsonl
    generate_stats: true
    stats_file: dataset_stats.json
    generate_plots: true
    plots_dir: data_stats/plots
