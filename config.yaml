run:
  n_turns: 2
  batch_size: 2
  total_samples: 1024
  min_turns: 2
  max_turns: 12
  turn_count_distribution: poisson
  turn_count_mean: 6.0
  target_languages: ["morocco", "msa", "gulf", "english", "levantine"] # list of target languages; total_samples is generated per language
  seed_question: ""
  question_seed: ""
  run_id: ""
  resume_run_id: ""
  seed_topics_path: data/seeds/topics.yaml
  seed_topics_variant: ""
  seed_topics_probability: 0.35
  seed_topics_enabled: false
  distributed:
    enabled: false
    executor: ray
    spawn:
      coordinator: true
      workers: true

models:
  embedding_model: google/embeddinggemma-300m
  use_reranker: false
  reranker_model: Qwen/Qwen3-Reranker-4B
  reranker_backend: qwen3
  reranker_instruction: >
    Given a user question and a candidate passage from the knowledge base,
    decide whether the passage contains information needed to answer the question.
  reranker_max_length: 8192
  qwen3_reranker_cmd: ""
  reranker_candidates: 12
  reranker_batch_size: 16

llm:
  backend: vllm_attach # openai | vllm_attach | vllm_managed
  provider: "openai"
  model: "" # override with OPENAI_MODEL and will be used as default for all agents unless specified in agents section
  # base_url: "https://api.openai.com/v1" # for openai API
  base_url: "http://localhost:1234/v1" # for vllm_attach with LM Studio
  # base_url: "http://localhost:8000/v1" # for vllm_managed with vLLM
  api_key: ""
  api_key_env: ""
  temperature: null
  max_tokens: null
  top_p: null
  timeout: null
  max_retries: null
  extra: {}
  routing:
    strategy: weighted_least_inflight
    # required for llm.backend=vllm_attach when you already have a vllm cluster running; not needed for llm.backend=vllm_managed as the cluster will be created and managed by dlgforge, not needed when llm.backend=openai
    endpoints: ["http://localhost:1234"] # for LM Studio
    # endpoints: ["http://localhost:8000"] # for vLLM
  vllm:
    model: openai/gpt-oss-20b
    served_model_name: gpt_oss_20b
    replicas: 1
    num_gpus_per_replica: 1
    host: 0.0.0.0
    advertise_host: 127.0.0.1
    port_start: 18000
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_num_seqs: 256
    health_timeout_s: 180
    auto_stop_on_exit: true
  agents:
    qa_generator:
      model: "openai/gpt-oss-20b" # gpt-5.2
    kb_responder:
      model: "openai/gpt-oss-20b" # gpt-5.2
    qa_judge: # qa_judge is ignored when judge.mode: offline in config.yaml.
      model: "openai/gpt-oss-20b" #gpt-5.2

ray:
  address: auto
  auto_start_local: true # when true and ray.address=auto finds no cluster, dlgforge starts a local Ray runtime
  namespace: dlgforge
  actor:
    num_cpus: 1.0
    num_gpus: 0.0
    coordinator_num_cpus: 1.0
    replicas_qa: 2
    replicas_complete: 2

store:
  backend: postgres
  postgres:
    dsn: ""
    min_pool_size: 5
    max_pool_size: 30
    statement_timeout_ms: 30000


retrieval:
  default_k: 4
  chunk_size: 750
  overlap: 150
  persist_dir: knowledge_index
  rebuild_index: false
  skip_if_unchanged: true
  embedding_backend: sentence_transformers
  embedding_model_kwargs: {}
  embedding_tokenizer_kwargs: {}
  embedding_encode_kwargs: {}

coverage:
  doc_coverage_mode: balanced
  doc_coverage_epsilon: 0.15
  doc_coverage_fraction: 0.2
  question_dedup_retries: 3

tools:
  web_search_enabled: false
  serper_num_results: 5
  serper_timeout: 30

personas:
  enabled: true
  path: src/dlgforge/prompts/personas.yaml

judge:
  enabled: true
  mode: online # online will make the judging in generation time, offline will not and will run separatly
  granularity: turn # turn or conversation
  reasons:
    - irrelevant
    - incorrect
    - hallucinated
    - weak_grounding
    - vague
    - incomplete
    - unsafe
    - other

saving:
  output_dir: outputs_local
  output_columns:
    messages: messages
    messages_with_tools: messages_with_tools
    metadata: metadata
    user_reasoning: user_reasoning
    assistant_reasoning: assistant_reasoning
    judge: judge
  hf_push:
    enabled: true
    auto_push_on_run: true
    # repo_id: MBZUAI-Paris/Synthetic-QA-Reasoning-Multilingual
    repo_id: MBZUAI-Paris/Synthetic-QA-Reasoning-local
    repo_type: dataset
    export_dir: hf_export
    include_run_state: false
    private: true
    commit_message: Update synthetic dataset export
    source_file: conversations_sharegpt_judged.jsonl
    # source_file: conversations_sharegpt.jsonl
    generate_stats: true
    stats_file: dataset_stats.json
    generate_plots: true
    plots_dir: data_stats/plots
